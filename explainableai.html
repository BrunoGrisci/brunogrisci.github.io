<!DOCTYPE html>
<html lang="pt-BR">
<head>

    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-T82L645Q');</script>
    <!-- End Google Tag Manager -->

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Explainable AI - Generative AI Academy</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #ffffff;
            color: #333;
            line-height: 1.6;
            margin: 0;
            padding: 40px;
        }
        h1, h2, h3 {
            color: #005A9C;
        }
        h1 {
            border-bottom: 3px solid #005A9C;
            padding-bottom: 10px;
        }
        h2 {
            margin-top: 40px;
        }
        ul {
            margin-left: 20px;
        }
        .container {
            max-width: 900px;
            margin: auto;
        }
        .footer {
            margin-top: 50px;
            font-style: italic;
            color: #555;
        }
        .contact a { color: #005A9C; text-decoration: none; }
        .contact a:hover { text-decoration: underline; }
        .tag {
            display: inline-block;
            background: #E6F0FA;
            color: #005A9C;
            padding: 2px 8px;
            border-radius: 999px;
            font-size: 0.9rem;
            margin-left: 8px;
        }
    </style>

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-PG6S1LBB4Y"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());

          gtag('config', 'G-PG6S1LBB4Y');
        </script>

</head>
<body>

    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T82L645Q"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->

    <div class="container">
        <h1>Explainable AI</h1>
        <h2>Generative AI Academy</h2>
        <h3>Prof. Bruno Iochins Grisci</h3>
        <div class="contact">
            E‑mail: <a href="mailto:bigrisci@inf.ufrgs.br">bigrisci@inf.ufrgs.br</a><br>
            Site: <a href="https://brunogrisci.github.io/" target="_blank" rel="noopener">https://brunogrisci.github.io/</a>
        </div>

        <h2>Descrição</h2>
        <p><strong>[T8] Explainable AI</strong></p>
        <p>O minicurso apresenta os fundamentos da interpretação de modelos de aprendizado de máquina, abordando tanto modelos naturalmente interpretáveis quanto técnicas modernas de explicação aplicáveis a modelos complexos. Serão discutidos métodos globais e locais, além de reflexões críticas sobre limitações e boas práticas no uso dessas ferramentas.</p>
        <p>Na parte prática, será realizada uma atividade em Python (Jupyter Notebook) com foco em modelos de árvore, abrangendo desde a preparação dos dados até a interpretação dos resultados. Os participantes aprenderão a manipular conjuntos de dados tabulares, explorar a influência de diferentes parâmetros no desempenho do modelo, extrair a relevância de características em estudo e visualizar os resultados por meio de gráficos, proporcionando uma compreensão prática de como construir, avaliar e interpretar modelos de árvore para análise de dados.</p>

        <h2>Materiais do Curso</h2>
        <ul>
            <li><a href="https://www.youtube.com/watch?v=QZdf6X4u5mU" target="_blank">Gravação: https://www.youtube.com/watch?v=QZdf6X4u5mU</a></li>
            <li><a href="https://brunogrisci.github.io/eramiaslides/interpretabilidade_brunogrisci.pdf" target="_blank">Slides (PDF): https://brunogrisci.github.io/eramiaslides/interpretabilidade_brunogrisci.pdf</a></li>
        </ul>

        <h2>Discussão</h2>
        <ul>
            <li>Em quais cenários é preferível utilizar modelos intrinsecamente interpretáveis em vez de explicadores pós-hoc como LIME ou SHAP?</li>
            <li>Como diferenciar explicações locais de explicações globais? Que tipos de perguntas cada abordagem responde melhor?</li>
            <li>Quais vantagens e limitações você identifica em métodos agnósticos ao modelo em comparação com métodos específicos de interpretabilidade?</li>
            <li>Por que explicações podem apresentar variância e instabilidade? Como isso afeta a confiança nas conclusões que tiramos?</li>
            <li>Como detectar quando um modelo aprendeu atalhos espúrios (Clever Hans) em vez de padrões reais no conjunto de dados?</li>
            <li>Qual é a diferença entre explicações baseadas em intervenção contrafactual e explicações baseadas em associações estatísticas?</li>
            <li>As propriedades dos valores de Shapley tornam o SHAP confiável em todos os cenários? Por quê?</li>
            <li>Como avaliar se uma explicação é útil, correta e acionável? Quais critérios são necessários em aplicações reais?</li>
            <li>Por que interpretar redes neurais profundas é particularmente difícil? Como métodos como LRP, CAM/Grad-CAM ou attribution graphs tentam mitigar esse desafio?</li>
            <li>Quais riscos estão associados ao uso indiscriminado de técnicas de interpretabilidade, especialmente em domínios sensíveis?</li>
        </ul>

        <h2>Quiz</h2>
        <a href="https://forms.gle" target="_blank">https://forms.gle/</a><br>

        <h2>Projeto</h2>
        <p>Notebook / atividade prática (fazer uma cópia para editar): <a href="https://colab.research.google.com/drive/1qVQoVDGOwbf0BL7R8LDh6cbt42rVUqmn?usp=sharing" target="_blank" rel="noopener">https://colab.research.google.com/drive/1qVQoVDGOwbf0BL7R8LDh6cbt42rVUqmn?usp=sharing</a></p>

        <h2>Bibliografia</h2>
        <ul>
            <li>Al‑Zawi, S. A. T. A. M. S.; Mohammed, T.; Albawi, S. (2017). "Understanding of a convolutional neural network." In: <em>ICET 2017</em>, pp. 1–6.</li>            
            <li>Aurélien Géron (2019). <em>Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>. 2ª edição.</li>
            <li>Bach, S. et al. (2015). "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation." <em>PLOS ONE</em>, 10(7), e0130140.</li>
            <li>Barbieri, M. C.; Grisci, B. I.; Dorn, M. (2024). "Analysis and comparison of feature selection methods towards performance and stability." <em>Expert Systems with Applications</em>, 249, 123667.</li>
            <li>Buhrmester, V.; Münch, D.; Arens, M. (2021). "Analysis of explainers of black box deep neural networks for computer vision: A survey." <em>Machine Learning and Knowledge Extraction</em>, 3(4), 966–989.</li>
            <li>Cooney, A.; Nanda, N. (2023). <em>Circuitsvis</em>.</li>
            <li>DeLMA; Cukierski, W. (2013). <em>The ICML 2013 Whale Challenge - Right Whale Redux</em>. Kaggle. Acessado em 09/03/2025.</li>
            <li>Domingos, P. (2012). "A few useful things to know about machine learning." <em>Communications of the ACM</em>, 55(10), 78–87.</li>
            <li>Elhage, N. et al. (2021). "A mathematical framework for transformer circuits." <em>Transformer Circuits Thread</em>, 1(1), 12.</li>
            <li>Geirhos, R. et al. (2020). "Shortcut learning in deep neural networks." <em>Nature Machine Intelligence</em>, 2(11), 665–673.</li>
            <li>Gurnee, W. et al. (2024). "Universal neurons in GPT‑2 language models." <em>arXiv preprint</em> arXiv:2401.12181.</li>
            <li>Grisci, B. I.; Inostroza-Ponta, M.; Dorn, M. (2025). "Assessing feature scorer results on high-dimensional datasets with t-SNE." <em>Neurocomputing</em>, p. 130561.</li>
            <li>Kumar, A. et al. (2025). "Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis." <em>arXiv preprint</em> arXiv:2505.11581.</li> 
            <li>Lapuschkin, S. et al. (2019). "Unmasking Clever Hans predictors and assessing what machines really learn." <em>Nature Communications</em>, 10, 1096.</li>
            <li>Lindsey, J. et al. (2025). <em>On the Biology of a Large Language Model</em>. Acessado em 11/11/2025. Disponível em: https://transformer-circuits.pub/2025/attribution-graphs/biology.html.</li>           
            <li>Miller, T. (2019). "Explanation in artificial intelligence: Insights from the social sciences." <em>Artificial Intelligence</em>, 267, 1–38.</li>
            <li>Molnar, C. (2020). <em>Interpretable Machine Learning</em>. Lulu.com.</li>
            <li>Molnar, C. (2025). <em>Points, Rules, Weights, Distributions: The Elements of Machine Learning</em>. Acessado em 09/11/2025.</li>
            <li>Montavon, G.; Samek, W.; Müller, K.-R. (2018). "Methods for interpreting and understanding deep neural networks." <em>Digital Signal Processing</em>, 73, 1–15.</li>
            <li>Murdoch, W. J. et al. (2019). "Definitions, methods, and applications in interpretable machine learning." <em>PNAS</em>, 116(44), 22071–22080.</li>
            <li>Olah, C.; Cammarata, N. et al. (2020). "Zoom In: An introduction to circuits." <em>Distill</em>, 5(3), e00024–001.</li>
            <li>Olah, C.; Mordvintsev, A.; Schubert, L. (2017). "Feature visualization." <em>Distill</em>, 2(11), e7.</li>
            <li>Prince, S. J. D. (2023). <em>Understanding Deep Learning</em>. MIT Press.</li>
            <li>Rai, D. et al. (2024). "A practical review of mechanistic interpretability for transformer-based language models." <em>arXiv preprint</em> arXiv:2407.02646.</li>
            <li>Ribeiro, M. T.; Singh, S.; Guestrin, C. (2016a). "Why should I trust you?" In: <em>Proc. SIGKDD</em>, pp. 1135–1144.</li>
            <li>Ribeiro, M. T.; Singh, S.; Guestrin, C. (2016b). "Model-agnostic interpretability of machine learning." <em>arXiv preprint</em> arXiv:1606.05386.</li>
            <li>Roscher, R. et al. (2020). "Explainable machine learning for scientific insights and discoveries." <em>IEEE Access</em>, 8, 42200–42216.</li>
            <li>Rudin, C. (2019). "Stop explaining black box machine learning models..." <em>Nature Machine Intelligence</em>, 1(5), 206–215.</li>
            <li>Scholbeck, C. A. et al. (2019). "Sampling, intervention, prediction, aggregation..." In: <em>ECML PKDD</em>. Springer, 205–216.</li>
            <li>Szegedy, C. et al. (2013). "Intriguing properties of neural networks." <em>arXiv preprint</em> arXiv:1312.6199.</li>
        </ul>

        <h2>Agradecimentos</h2>
        <h3>MSc. Débora Cristina Santos de Sousa</h3>
        <div class="contact">
            E‑mail: <a href="mailto:debora.sousa@inf.ufrgs.br">debora.sousa@inf.ufrgs.br</a><br>
            Site: <a href="http://lattes.cnpq.br/9112276170232610" target="_blank" rel="noopener">http://lattes.cnpq.br/9112276170232610</a>
        </div>

        <h2>Veja também</h2>
        <p>Feature Selection: <a href="https://brunogrisci.github.io/featureselection" target="_blank" rel="noopener">https://brunogrisci.github.io/featureselection</a></p>

        <div class="footer">
            <p>Generative AI Academy | Explainable AI | © Prof. Bruno Iochins Grisci | All rights reserved.</p>
        </div>
    </div>
</body>
</html>
